from nltk.tokenize import sent_tokenize
from collections import defaultdict
import argparse
import gzip
import sys
import numpy as np
import itertools
import re
import codecs
import json
'''

'''

parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input_file', required=True, help='input file in pubtator format')
parser.add_argument('-d', '--output_dir', required=True, help='write results to this dir')
parser.add_argument('-f', '--output_file_suffix', required=True, help='append this suffix to outfiles')
parser.add_argument('-s', '--max_seq', default=500, help='max sentence length')
parser.add_argument('-a', '--full_abstract', default=False, type=bool, help='full abstracts otherwise sentence segment')
parser.add_argument('-p', '--pubmed_filter', help='Only export abstracts/annotations from these pubmed ids')
parser.add_argument('-r', '--relations', help='two column list of relations to add to annotations')
parser.add_argument('-w', '--word_piece_codes', help='if supplied with codes generated by , uses word pieces to tokenize')
parser.add_argument('-t', '--shards', default=1, help='export data to this many different shards')
parser.add_argument('-x', '--export_all_eps', default=False, type=bool, help='export all ep interactions labeled in the data')
parser.add_argument('-n', '--export_negatives', default=False, type=bool, help='also export documents with no positive interactions')
parser.add_argument('-e', '--encoding', default=None, help='encoding for files')
parser.add_argument('-m', '--max_distance', default=-1, help='throw out examples with mentions too far apart')

args = parser.parse_args()
for arg in vars(args):
    print arg, getattr(args, arg)

# some initial values
current_pub = ''
total_annotations = 0
valid_pubs = 0
pos_interactions = 0
pos_interactions_gold = 0
exported_triples = 0
ENTITY_STRING = 'ENTITY_'
used_annotations = set()
export_types = ['Chemical::Disease']
if args.export_all_eps:
    export_types.append('Chemical::Gene')
    export_types.append('Gene::Disease')
export_types = set(export_types)
pubmed_filter, global_relations = None, None
if args.pubmed_filter:
    with codecs.open(args.pubmed_filter, 'r', args.encoding) as f:
        pubmed_filter = set([l.strip() for l in f])
if args.relations:
    with codecs.open(args.relations, 'r', args.encoding) as f:
        lines = [l.strip().split('\t') for l in f]
        global_relations = dict({(l[0], l[1], l[2]): 'CID' for l in lines}.items() +
                                {('MESH:%s' % l[0], 'MESH:%s' % l[1], l[2]): 'CID' for l in lines}.items() +
                                {('%s' % l[0], 'MESH:%s' % l[1], l[2]): 'CID' for l in lines}.items() +
                                {('MESH:%s' % l[0], '%s' % l[1], l[2]): 'CID' for l in lines}.items())
# either use word piece tokenizer or genia tokenizer
if args.word_piece_codes:
    from word_piece_tokenizer import WordPieceTokenizer
    wpt = WordPieceTokenizer(args.word_piece_codes, entity_str=ENTITY_STRING)
    tokenize = wpt.tokenize
else:
    import genia_tokenizer
    tokenize = genia_tokenizer.tokenize



def make_tsv_line(_e1, e1_mention, e1_type, _e2, e2_mention, e2_type, doc_id, seq_len,
                  e1_start, e1_end, e2_start, e2_end, text, arg_sentence=None, out_label=None):
    """
    generate an output line in 13 column tsv format
    """
    out_text = arg_sentence if arg_sentence is not None else text
    out_label = out_label if out_label is not None else 'Unk'
    out_line = '%s\t%s\t%s\t%s\t%s\t\
                %s\t%s\t%s\t%s\t%s\t\
                %s\t%s\t%s' \
               % (_e1.strip(), e1_type.strip(), e1_mention.strip(), e1_start.strip(), e1_end.strip(),
                  _e2.strip(), e2_type.strip(), e2_mention.strip(), e2_start.strip(), e2_end.strip(),
                  doc_id.strip(), out_label.strip(), out_text.strip())
    # out_line = re.sub(' +', ' ', out_line)
    out_line = re.sub('\\t +', '\t', out_line)
    return out_line+'\n'


def make_examples(doc_id, tokens, entities, filter_entities, label_map, export_arg_sentence=False):
    """
    iterate over entity pairs in this text and export examples
    """
    interactions = 0
    # split multi-token entities and enocde utf
    tokens = [item for sublist in tokens for item in sublist]
    _out_sentence = ' '.join(tokens)
    examples = []
    max_distance = int(args.max_distance)

    grouped_entities = defaultdict(list)
    for tup, _start, _end in entities:
        _mention, _type, _kg_id = tup
        grouped_entities[_kg_id].append((_mention, _type, _kg_id, _start, _end))

    pairs = 0
    for e1_mentions, e2_mentions in itertools.product(grouped_entities.values(), grouped_entities.values()):
        # skip if e1 and e2 are the same mention
        if e1_mentions != e2_mentions:
            pairs += 1
            e1_str = e1_mentions[0][0]
            e2_str = e2_mentions[0][0]
            e1_type = e1_mentions[0][1]
            e2_type = e2_mentions[0][1]
            e1_kg_id = e1_mentions[0][2]
            e2_kg_id = e2_mentions[0][2]
            ep_type = '::'.join([e1_type, e2_type])
            if ep_type in export_types:
                e1_starts = [_start for _, _, _, _start, _end in e1_mentions]
                e1_ends = ':'.join([_end for _, _, _, _start, _end in e1_mentions])
                e2_starts = [_start for _, _, _, _start, _end in e2_mentions]
                e2_ends = ':'.join([_end for _, _, _, _start, _end in e2_mentions])

                if max_distance > 0:
                    e1_s_ints = [int(e1_s) for e1_s in e1_starts]
                    e2_s_ints = [int(e2_s) for e2_s in e2_starts]
                    min_distance = min([(max(e1_s, e2_s)-min(e1_s, e2_s)) for e1_s in e1_s_ints for e2_s in e2_s_ints])
                if max_distance <= 0 or min_distance <= max_distance:
                    if label_map is not None and (e1_kg_id, e2_kg_id, doc_id) in label_map:
                        out_label = label_map[(e1_kg_id, e2_kg_id, doc_id)]
                        interactions += 1
                    elif global_relations is not None and (e1_kg_id, e2_kg_id, doc_id) in global_relations:
                        out_label = global_relations[(e1_kg_id, e2_kg_id, doc_id)]
                        interactions += 1
                    else:
                        out_label = 'Null'

                    example = make_tsv_line(e1_kg_id, e1_str, e1_type, e2_kg_id, e2_str, e2_type, doc_id, len(tokens),
                                            ':'.join(e1_starts), e1_ends, ':'.join(e2_starts), e2_ends,
                                            _out_sentence, out_label=out_label)
                    examples.append((example, out_label))
    return examples, interactions


def make_ner_examples(doc_id, tokens, entities, out_file, filter_entities, label_map):
    starts = {int(start): (int(end), _type, _kg_id) for (_mention, _type, _kg_id), start, end in entities}
    tokens_fixed = [item for sublist in tokens for item in sublist]

    last_end = 0
    ner_examples = []
    for idx, t in enumerate(tokens_fixed):
        bio = ''
        kg_id = '-1'
        if idx in starts:
            last_end, e_type, kg_id = starts[idx]
            bio = 'B-'
        if idx >= last_end:
            e_type = 'O'
        elif idx not in starts:
            bio = 'I-'
        example = '%s\t%s%s\t%s\t%s\n' % (t, bio, e_type, kg_id, doc_id)
        out_file.write(example)
    out_file.write('\n')
    return ner_examples

"""
File (in_f) 읽기 시작 : x if 조건 else y 문법 사용해서 알집인지 알집해제되어있는지 확인해서 읽음.
done, line_num, last_print, print_every : 파일 내의 몇 줄에 한번씩 출력될건지.. 뭐 이런거에 관련된 변수들이라 크게 신경쓰지 않아도 괜찮을 것 같다. 

"""
in_f = gzip.open(args.input_file, 'rb') if args.input_file.endswith('gz') else codecs.open(args.input_file, 'r', args.encoding)
ner_out_file = codecs.open('%s/ner_%s' % (args.output_dir, args.output_file_suffix), 'w', args.encoding)
positive_outs = [codecs.open('%s/positive_%d_%s' % (args.output_dir, i, args.output_file_suffix), 'w', args.encoding)
                 for i in range(0, int(args.shards))]
negative_outs = [codecs.open('%s/negative_%d_%s' % (args.output_dir, i, args.output_file_suffix), 'w', args.encoding)
                 for i in range(0, int(args.shards))]
done = False
# with open(args.output_file, 'w') as out_f:
line_num = 0
last_print = 500
print_every = 500

while not done:
    try:
        """
        출력부분
        """
        if line_num > last_print:
            sys.stdout.write('\rpubs %d annotations: %d lines %d  examples :%d pos_examples: %d'
                         % (valid_pubs, total_annotations, line_num, exported_triples, pos_interactions))
            sys.stdout.flush()
            last_print = line_num + print_every

        """
        in_f 파일 내부는, 


        227508|t|Naloxone reverses the antihypertensive effect of clonidine.
        227508|a|In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibited or reversed by nalozone, 0.2 to 2 mg/kg. The hypotensive effect of 100 mg/kg alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood pressure or heart rate. In brain membranes from spontaneously hypertensive rats clonidine, 10(-8) to 10(-5) M, did not influence stereoselective binding of [3H]-naloxone (8 nM), and naloxone, 10(-8) to 10(-4) M, did not influence clonidine-suppressible binding of [3H]-dihydroergocryptine (1 nM). These findings indicate that in spontaneously hypertensive rats the effects of central alpha-adrenoceptor stimulation involve activation of opiate receptors. As naloxone and clonidine do not appear to interact with the same receptor site, the observed functional antagonism suggests the release of an endogenous opiate by clonidine or alpha-methyldopa and the possible role of the opiate in the central control of sympathetic tone.
        227508	0	8	Naloxone	Chemical	D009270
        227508	93	105	hypertensive	Disease	D006973
        227508	671	695	[3H]-dihydroergocryptine	Chemical	-1
        227508	750	762	hypertensive	Disease	D006973
        227508	1026	1035	clonidine	Chemical	D003000
        227508	1039	1055	alpha-methyldopa	Chemical	D008750
        227508	CID	D008750	D007022        
        ...(중략)...

        (한줄띄고)
        354896|t|Lidocaine-induced cardiac asystole.
        354896|a|Intravenous administration of a single 50-mg bolus of lidocaine in a 67-year-old man resulted in profound depression of the activity of the sinoatrial and atrioventricular nodal pacemakers. The patient had no apparent associated conditions which might have predisposed him to the development of bradyarrhythmias; and, thus, this probably represented a true idiosyncrasy to lidocaine.
        354896	0	9	Lidocaine	Chemical	D008012
        354896	18	34	cardiac asystole	Disease	D006323
        354896	90	99	lidocaine	Chemical	D008012
        ...(생략)...
        


        이런식으로 생겼기 때문에, 일단 첫 두줄을 읽어서 title 과 abstract를 저장한 후에 그다음엔 (한줄띄고) 가 나올때까지 매 줄 읽어가면서 Entity 정보와 Relation 정보 수집.
        """
        line = in_f.readline().strip()
        pub_id, _, title_list = line.split('|', 2)
        line_num += 1
        title = ''.join(title_list)
        abstract = ''.join(in_f.readline().strip().split('|')[2:])
        line_num += 1
        current_pub = '%s %s' % (title, abstract)

        """
        본격적으로 한 논문에 대한 (한줄띄고가 나오기 전까지에 대한) Entity 정보와 Relation 정보 모으는 부분. 특히 \t 를 기준으로 쪼갰을때 개수가 몇개인지를 통해 Entity 정보인지 Relation 정보인지 구분.
        """
        label_annotations = {}
        current_annotations = []
        try:
            line = in_f.readline().strip()
            line_num += 1
            while line:
                parts = line.split('\t')
                if len(parts) == 4:
                    pub_id, rel, e1, e2 = parts
                    label_annotations[(e1, e2, pub_id)] = rel
                    pos_interactions_gold += 1
                elif len(parts) == 6:
                    pub_id, start, end, mention, label, kg_ids = parts
                    for kg_id in kg_ids.split('|'):
                        current_annotations.append((pub_id, int(start), int(end), mention, label, kg_id))
                        total_annotations += 1
                elif len(parts) == 7:
                    pub_id, start, end, mention, label, kg_ids, split_mentions = parts
                    for kg_id in kg_ids.split('|'):
                        current_annotations.append((pub_id, int(start), int(end), mention, label, kg_id))
                        total_annotations += 1
                # else:
                #     print('weird number of fields for annotations: %s' % line)
                line = in_f.readline().strip()
                line_num += 1

            """
            while line: 이 끝나는 때는 (한줄띄고) 가 나타났을때, 즉 한 논문에 대한 Entity 와 Relation 정보가 다 읽혔을 때 이므로, 이때 pubmed_filter 라는 argparser를 통해 입력받은 값에 따라 
            본격적으로 tokenize 시작.. (pubmed filter의 역할은 아마 특정 pub_id 를 가진 논문만 tokenize 하는 것인 듯 하다. 그런데 pubmed_filter가 없으면 그냥 모두 tokenize.)
            """
            if not pubmed_filter or pub_id in pubmed_filter:
                # do something with last annotations
                valid_pubs += 1

                """
                current_annotations 라는, 위에서 Entity 정보 읽을때 저장해둔 Entity 에 대한 list를 start 를 기준으로 정렬(먼저 나오는 Entity 순으로 정렬)
                """
                sorted_annotations = sorted(current_annotations, key=lambda tup: tup[1])
                replaced_text = []
                last = 0
                annotation_map = {}

                """
                각 Entity 가 존재하는 위치에 따라 정렬되었으니까, 앞에서부터 그 entity 가 존재하는 부분을 ENTITY_1_ 과 같은 꼴로 변환해서 replaced_text에 추가.
                ENTITY_1_ 의 원래 Entity 정보가 뭔지 annotation_map 이라는 dictionary 에 저장. 
                """
                for i, (pub_id, start, end, mention, label, kg_id) in enumerate(sorted_annotations):
                    mention = current_pub[start:end]
                    dummy_token = '%s%d_' % (ENTITY_STRING, i)
                    replaced_text.append(' %s %s ' % (current_pub[last:start].strip(), dummy_token))
                    last = end
                    annotation_map[dummy_token] = (mention, label, kg_id)

                # add text that occurs after the last entity
                replaced_text.append(current_pub[end:])

                """
                완성된 replaced_text 리스트를 통해 변환된 abstract 완성
                full_abstract 라는 argparser 값에 따라 abstract를 각 문장별로 처리할지 통째로 처리할지 결정하는데, 보내준 명령어에는 통째로 처리하는 명령어로 되어있었다.
                entity_counts는 abstract에 등장한 entity 의 개수(중복까지 포함해서 센다).
                """

                abstract = ''.join(replaced_text).replace('  ', ' ')
                sentences = [abstract] if args.full_abstract else sent_tokenize(abstract)
                entity_counts = [(s.count(ENTITY_STRING), s) for s in sentences]

                """
                여기서 각 문장, 또는 abstract 통째로 tokenize를 통해 @@ 을 사용해 쪼개지는데, 아직은 ENTITY_1_ 처럼 표시되어있다.
                """
                tokenized_sentences = [[w for w in tokenize(s)] for s in sentences]


                """
                ENTITY_1_ 와 같은 entity 들을 제외하고 @를 이용해 쪼개진 token의 개수가 argparser에서 입력받은 max_seq보다 적은 경우만 골라서 (위에서 계속 언급하듯 지금은 abstract 통째로 실행하도록 명령어가 되어있어서
                그냥 abstract 에서 entity 를 제외한 모든 단어들이 쪼개졌을때 개수가 max_seq 이하인지 검사한다고 봐도 무방), 다시 ENTITY_1_ 을 원래 entity의 원본이름(label 말고.. abstract에 등장했던 그대로) 으로 고쳐서 tokenize.
                """
                valid_tokenized_sentences = [s for s in tokenized_sentences if len(s) <= args.max_seq]
                out_sentence = [[tokenize(annotation_map[token][0]) if token.startswith(ENTITY_STRING) else [token]
                                 for token in sentence]
                                for sentence in valid_tokenized_sentences]


                """
                각 단위(abstract 통째 or 문장별로) token 개수와 누적개수 측정인데, 출력해보기전까지 아래꺼랑 이거는 뭔지 모르겠다.
                """
                # get the token offsets in the sentence for each entity
                token_lens = [[len(token) if type(token) is list else 1 for token in sentence] for sentence in out_sentence]
                token_offsets = [np.cumsum(lens) for lens in token_lens]

                """
                entity offset에 대한 정보를 위의 token 개수와 누적개수로 만들어주는데, 출력해보기전까지 위에꺼랑 이거는 뭔지 모르겠다.
                """
                entity_offsets = [[(annotation_map[token], str(offset[i] - length[i]), str(offset[i]))
                                   for (i, token) in enumerate(sentence) if token.startswith(ENTITY_STRING)]
                                  for length, offset, sentence
                                  in zip(token_lens, token_offsets, valid_tokenized_sentences)]

                """
                make_examples 함수를 이용하여 이쁜 형태의 문자열로 만들고
                """
                out_examples = [make_examples(pub_id, sent, offset, filter_entities=None, label_map=label_annotations)
                                for sent, offset in zip(out_sentence, entity_offsets)]

                """
                앞에 정의해둔 파일들에 저장.
                """
                for ex_pos_count in out_examples:
                    ex_label, pos_count = ex_pos_count
                    if ex_label and (pos_count > 0 or args.export_negatives):
                        # ex, out_labels = zip(*ex)
                        pos_interactions += pos_count
                        exported_triples += len(ex_label)
                        positive_examples = [ex for ex, out_label in ex_label if out_label != 'Null']
                        negative_examples = [ex for ex, out_label in ex_label if out_label == 'Null']
                        shard_id = np.random.randint(0, int(args.shards))
                        if positive_examples: positive_outs[shard_id].write((''.join(positive_examples)))
                        if negative_examples: negative_outs[shard_id].write((''.join(negative_examples)))
                if ner_out_file:
                    [make_ner_examples(pub_id, sent, offset, ner_out_file, filter_entities=None, label_map=label_annotations)
                     for sent, offset in zip(out_sentence, entity_offsets)]

        except Exception as e:
            print(line_num)
            print('Error: %s' % e)
    except Exception as e2:
        done = True
        in_f.close()
        ner_out_file.close()
        for i in range(0, int(args.shards)):
            positive_outs[shard_id].close()
            negative_outs[shard_id].close()

sys.stdout.write('\rpubs %d  annotations: %d   lines %d  examples :%d  pos_examples: %d   pos_example_annotations %d\n'
      % (valid_pubs, total_annotations, line_num, exported_triples, pos_interactions, pos_interactions_gold))
sys.stdout.flush()
